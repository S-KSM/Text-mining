{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project Dave!\n",
    "\n",
    "\n",
    "## How the inputs should be?\n",
    "\n",
    "The Keywords should be in different lines of a ** *.dat** file. In order to achieve this, just open a text editor, NotePad++ works very nice on Windows and it is free.\n",
    "\n",
    "You documents are supposed to be a ** *.txt ** file.\n",
    "\n",
    "The files that needs to be analyzed, should be in the same folder as your code.\n",
    "\n",
    "    The code lines are commented to show what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "path_documents = \"*.txt\"\n",
    "path_inp = \"*.dat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### function that finds the occurance of a keyword in a line.\n",
    "def findSectionOffsets(text,pattern):\n",
    "    indexes = []\n",
    "    startposition = 0\n",
    "    text = str(text)\n",
    "    pattern = str(pattern)\n",
    "    x = True\n",
    "\n",
    "    while x == True:\n",
    "        i = text.find(pattern, startposition)\n",
    "        if i == -1: x = False\n",
    "        indexes.append(i)\n",
    "        startposition = i + 1\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "### Reads Keywords from your file\n",
    "for file in glob.glob(path_inp):\n",
    "    f = open(file,\"r\",encoding='utf-8')\n",
    "    inp = f.read()\n",
    "    keywords = [line for line in inp.splitlines()] # makes a list of keywords\n",
    "    \n",
    "stop_words = set(stopwords.words('english')) ### Make a list of stopwords            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's get started.\n",
      "Analyzing: 05_Tajikistan_1994_May_all.txt\n",
      "Analyzing: 08_Tajikistan_1994_August_001400.txt\n",
      "Analyzing: 09_Tajikistan_1994_August_401524.txt\n",
      "Analyzing: 11_Tajikistan_1994_September_301633.txt\n",
      "Analyzing: 12_Tajikistan_1994_October_001400.txt\n",
      "Analyzing: 13_Tajikistan_1994_October_401632.txt\n",
      "Let's get started.\n",
      "Analyzing: 05_Tajikistan_1994_May_all.txt\n",
      "Analyzing: 08_Tajikistan_1994_August_001400.txt\n",
      "Analyzing: 09_Tajikistan_1994_August_401524.txt\n",
      "Analyzing: 11_Tajikistan_1994_September_301633.txt\n",
      "Analyzing: 12_Tajikistan_1994_October_001400.txt\n",
      "Analyzing: 13_Tajikistan_1994_October_401632.txt\n",
      "Let's get started.\n",
      "Analyzing: 05_Tajikistan_1994_May_all.txt\n",
      "Analyzing: 08_Tajikistan_1994_August_001400.txt\n",
      "Analyzing: 09_Tajikistan_1994_August_401524.txt\n",
      "Analyzing: 11_Tajikistan_1994_September_301633.txt\n",
      "Analyzing: 12_Tajikistan_1994_October_001400.txt\n",
      "Analyzing: 13_Tajikistan_1994_October_401632.txt\n",
      "Let's get started.\n",
      "Analyzing: 05_Tajikistan_1994_May_all.txt\n",
      "Analyzing: 08_Tajikistan_1994_August_001400.txt\n",
      "Analyzing: 09_Tajikistan_1994_August_401524.txt\n",
      "Analyzing: 11_Tajikistan_1994_September_301633.txt\n",
      "Analyzing: 12_Tajikistan_1994_October_001400.txt\n",
      "Analyzing: 13_Tajikistan_1994_October_401632.txt\n",
      "1 loops, best of 3: 17.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "### total is a dictionary that is going to store everything\n",
    "print(\"Let's get started.\")\n",
    "total = {}\n",
    "dict_top100 = {}\n",
    "final_wordfrequency = {}\n",
    "for file in glob.glob(path_documents):\n",
    "    total[file]={}\n",
    "    print(\"Analyzing: \" + str(file))\n",
    "\n",
    "    \n",
    "    f = open(file,\"r\",errors = \"surrogateescape\") # In case there is an error with the formatting, it tries to handle\n",
    "    myfile = f.read() #Reads the file\n",
    "    mysentences = sent_tokenize(myfile) # extract sentences\n",
    "    mywords = word_tokenize(myfile) # extract words\n",
    "    \n",
    "    filtered_mywords = [w for w in mywords if not w in stop_words]\n",
    "    #dict_top100[file] = Counter(filtered_mywords).most_common(100)\n",
    "    \n",
    "    #cnt = Counter()\n",
    "    \n",
    "    #for word in filtered_mywords:\n",
    "    #    if word in keywords:\n",
    "    #        cnt[word] += 1\n",
    "            \n",
    "    #print (cnt)\n",
    "    #final_wordfrequency[file] = cnt\n",
    "\n",
    "    #print(len(mysentences))\n",
    "    \n",
    "    for i in range(len(mysentences)):\n",
    "        #print(type(line))\n",
    "        line = mysentences[i]\n",
    "\n",
    "        \n",
    "        for key in keywords:\n",
    "            if key not in total[file]:\n",
    "                total[file][key]=[]\n",
    "            else:\n",
    "                #print(1)\n",
    "                indices = findSectionOffsets(str(line),str(key))\n",
    "                \n",
    "                if len(indices) > 1:\n",
    "                    \n",
    "                    phrase = line[indices[0]-50:indices[0]+50+len(key)]\n",
    "                    \n",
    "                        \n",
    "                    if len(phrase)-len(key) < 100: \n",
    "                        try:\n",
    "                            if len(mysentences[i+1]) > 100-len(phrase)-len(key):\n",
    "                                phrase += mysentences[i+1][100-len(phrase)-len(key)]\n",
    "                                #print(len(phrase))\n",
    "                            else:\n",
    "                                #mysentences[i+1]\n",
    "                                phrase += mysentences[i+1]\n",
    "                                phrase += mysentences[i+2][100 - len(phrase) - len(key)]\n",
    "                        except:\n",
    "                            pass\n",
    "                    #for i in range(len(indices)):\n",
    "                    total[file][key].append(phrase)\n",
    "\n",
    "            \n",
    "df = pd.DataFrame(total)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "final_output = open(\"finaloutput.csv\",\"w\")\n",
    "final_output.write(\"FileName, Keyword, Sentence\\n\")\n",
    "for keyword in keywords:\n",
    "    col_key = keyword\n",
    "    for column in df.columns:\n",
    "        col_file = column\n",
    "        for i in range(len(total[column][keyword])):\n",
    "            ### we are removing the commas and next line as well as byte characters for readability of the output\n",
    "            sentence = str(total[column][keyword][i].replace(\"\\n\",\" \").replace(\",\",\" \")).encode('ascii','ignore')\n",
    "            sentence = sentence.decode(\"utf-8\")\n",
    "            if len(sentence) > 20:\n",
    "                final_output.write(col_file + \", \" + col_key + \", \" + sentence + \"\\n\")\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "final_output.close()\n",
    "\n",
    "### write the output of the frequency of each keyword in files\n",
    "#final_wordfreq = open(\"finalwordfreq_Updated.csv\",\"w\") #output file is opened to be written in\n",
    "#final_wordfreq.write(\"FileName, Keyword, Frequency\\n\") # column names of the csv file\n",
    "#for keyword in keywords:\n",
    "#    col_key = keyword\n",
    "#    #print(col_key)\n",
    "#    for column in df.columns:\n",
    "#        col_file = column\n",
    "#        #print(col_file)\n",
    "#        frequency = final_wordfrequency[column][col_key]\n",
    "#        if frequency > 0:\n",
    "#            final_wordfreq.write(col_file + \", \" + col_key + \", \" + str(frequency) + \"\\n\")\n",
    "        \n",
    "#final_wordfreq.close()\n",
    "df_temp = pd.read_csv(\"finaloutput.csv\")\n",
    "grouped = df_temp.groupby([' Keyword','FileName'])\n",
    "df_temp2 = grouped.count()\n",
    "df_temp2.columns = ['Frequency']\n",
    "df_temp2.to_csv(\"Frequency_updated.csv\")\n",
    "\n",
    "print(\"Bye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
